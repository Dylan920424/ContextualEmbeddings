{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZNveKzvsPaV",
        "outputId": "10640b3f-5a5c-42b1-bbe5-b121b9adb9cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdvORXhU0E4E"
      },
      "outputs": [],
      "source": [
        "%pip install pyxdameraulevenshtein\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import difflib\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    from pyxdameraulevenshtein import damerau_levenshtein_distance_withNPArray\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "\n",
        "class Corpus():\n",
        "    _keys_frequency = None\n",
        "\n",
        "    def __init__(self, out_of_vocabulary=-1, skip=-2):\n",
        "        \"\"\" The Corpus helps with tasks involving integer representations of\n",
        "        words. This object is used to filter, subsample, and convert loose\n",
        "        word indices to compact word indices.\n",
        "\n",
        "        'Loose' word arrays are word indices given by a tokenizer. The word\n",
        "        index is not necessarily representative of word's frequency rank, and\n",
        "        so loose arrays tend to have 'gaps' of unused indices, which can make\n",
        "        models less memory efficient. As a result, this class helps convert\n",
        "        a loose array to a 'compact' one where the most common words have low\n",
        "        indices, and the most infrequent have high indices.\n",
        "\n",
        "        Corpus maintains a count of how many of each word it has seen so\n",
        "        that it can later selectively filter frequent or rare words. However,\n",
        "        since word popularity rank could change with incoming data the word\n",
        "        index count must be updated fully and `self.finalize()` must be called\n",
        "        before any filtering and subsampling operations can happen.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        out_of_vocabulary : int, default=-1\n",
        "            Token index to replace whenever we encounter a rare or unseen word.\n",
        "            Instead of skipping the token, we mark as an out of vocabulary\n",
        "            word.\n",
        "        skip : int, default=-2\n",
        "            Token index to replace whenever we want to skip the current frame.\n",
        "            Particularly useful when subsampling words or when padding a\n",
        "            sentence.\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> corpus = Corpus()\n",
        "        >>> words_raw = np.random.randint(100, size=25)\n",
        "        >>> corpus.update_word_count(words_raw)\n",
        "        >>> corpus.finalize()\n",
        "        >>> words_compact = corpus.to_compact(words_raw)\n",
        "        >>> words_pruned = corpus.filter_count(words_compact, min_count=2)\n",
        "        >>> # words_sub = corpus.subsample_frequent(words_pruned, thresh=1e-5)\n",
        "        >>> words_loose = corpus.to_loose(words_pruned)\n",
        "        >>> not_oov = words_loose > -1\n",
        "        >>> np.all(words_loose[not_oov] == words_raw[not_oov])\n",
        "        True\n",
        "        \"\"\"\n",
        "        self.counts_loose = defaultdict(int)\n",
        "        self._finalized = False\n",
        "        self.specials = dict(out_of_vocabulary=out_of_vocabulary,\n",
        "                             skip=skip)\n",
        "\n",
        "    @property\n",
        "    def n_specials(self):\n",
        "        return len(self.specials)\n",
        "\n",
        "    def update_word_count(self, loose_array):\n",
        "        \"\"\" Update the corpus word counts given a loose array of word indices.\n",
        "        Can be called multiple times, but once `finalize` is called the word\n",
        "        counts cannot be updated.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        loose_array : int array\n",
        "            Array of word indices.\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> corpus = Corpus()\n",
        "        >>> corpus.update_word_count(np.arange(10))\n",
        "        >>> corpus.update_word_count(np.arange(8))\n",
        "        >>> corpus.counts_loose[0]\n",
        "        2\n",
        "        >>> corpus.counts_loose[9]\n",
        "        1\n",
        "        \"\"\"\n",
        "        self._check_unfinalized()\n",
        "        uniques, counts = np.unique(np.ravel(loose_array), return_counts=True)\n",
        "        msg = \"Loose arrays cannot have elements below the values of special \"\n",
        "        msg += \"tokens as these indices are reserved\"\n",
        "        assert uniques.min() >= min(self.specials.values()), msg\n",
        "        for k, v in zip(uniques, counts):\n",
        "            self.counts_loose[k] += v\n",
        "\n",
        "    def _loose_keys_ordered(self):\n",
        "        \"\"\" Get the loose keys in order of decreasing frequency\"\"\"\n",
        "        loose_counts = sorted(self.counts_loose.items(), key=lambda x: x[1],\n",
        "                              reverse=True)\n",
        "        keys = np.array(loose_counts)[:, 0]\n",
        "        counts = np.array(loose_counts)[:, 1]\n",
        "        order = np.argsort(counts)[::-1].astype('int32')\n",
        "        keys, counts = keys[order], counts[order]\n",
        "        # Add in the specials as a prefix to the other keys\n",
        "        specials = np.sort(self.specials.values())\n",
        "        keys = np.concatenate((specials, keys))\n",
        "        empty = np.zeros(len(specials), dtype='int32')\n",
        "        counts = np.concatenate((empty, counts))\n",
        "        n_keys = keys.shape[0]\n",
        "        assert counts.min() >= 0\n",
        "        return keys, counts, n_keys\n",
        "\n",
        "    def finalize(self):\n",
        "        \"\"\" Call `finalize` once done updating word counts. This means the\n",
        "        object will no longer accept new word count data, but the loose\n",
        "        to compact index mapping can be computed. This frees the object to\n",
        "        filter, subsample, and compactify incoming word arrays.\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> corpus = Corpus()\n",
        "        >>> # We'll update the word counts, making sure that word index 2\n",
        "        >>> # is the most common word index.\n",
        "        >>> corpus.update_word_count(np.arange(1) + 2)\n",
        "        >>> corpus.update_word_count(np.arange(3) + 2)\n",
        "        >>> corpus.update_word_count(np.arange(10) + 2)\n",
        "        >>> corpus.update_word_count(np.arange(8) + 2)\n",
        "        >>> corpus.counts_loose[2]\n",
        "        4\n",
        "        >>> # The corpus has not been finalized yet, and so the compact mapping\n",
        "        >>> # has not yet been computed.\n",
        "        >>> corpus.keys_counts[0]\n",
        "        Traceback (most recent call last):\n",
        "            ...\n",
        "        AttributeError: Corpus instance has no attribute 'keys_counts'\n",
        "        >>> corpus.finalize()\n",
        "        >>> corpus.n_specials\n",
        "        2\n",
        "        >>> # The special tokens are mapped to the first compact indices\n",
        "        >>> corpus.compact_to_loose[0]\n",
        "        -2\n",
        "        >>> corpus.compact_to_loose[0] == corpus.specials['skip']\n",
        "        True\n",
        "        >>> corpus.compact_to_loose[1] == corpus.specials['out_of_vocabulary']\n",
        "        True\n",
        "        >>> corpus.compact_to_loose[2]  # Most popular token is mapped next\n",
        "        2\n",
        "        >>> corpus.loose_to_compact[3]  # 2nd most popular token is mapped next\n",
        "        4\n",
        "        >>> first_non_special = corpus.n_specials\n",
        "        >>> corpus.keys_counts[first_non_special] # First normal token\n",
        "        4\n",
        "        \"\"\"\n",
        "        # Return the loose keys and counts in descending count order\n",
        "        # so that the counts arrays is already in compact order\n",
        "        self.keys_loose, self.keys_counts, n_keys = self._loose_keys_ordered()\n",
        "        self.keys_compact = np.arange(n_keys).astype('int32')\n",
        "        self.loose_to_compact = {l: c for l, c in\n",
        "                                 zip(self.keys_loose, self.keys_compact)}\n",
        "        self.compact_to_loose = {c: l for l, c in\n",
        "                                 self.loose_to_compact.items()}\n",
        "        self.specials_to_compact = {s: self.loose_to_compact[i]\n",
        "                                    for s, i in self.specials.items()}\n",
        "        self.compact_to_special = {c: s for c, s in\n",
        "                                   self.specials_to_compact.items()}\n",
        "        self._finalized = True\n",
        "\n",
        "    @property\n",
        "    def keys_frequency(self):\n",
        "        if self._keys_frequency is None:\n",
        "            f = self.keys_counts * 1.0 / np.sum(self.keys_counts)\n",
        "            self._keys_frequency = f\n",
        "        return self._keys_frequency\n",
        "\n",
        "    def _check_finalized(self):\n",
        "        msg = \"self.finalized() must be called before any other array ops\"\n",
        "        assert self._finalized, msg\n",
        "\n",
        "    def _check_unfinalized(self):\n",
        "        msg = \"Cannot update word counts after self.finalized()\"\n",
        "        msg += \"has been called\"\n",
        "        assert not self._finalized, msg\n",
        "\n",
        "    def filter_count(self, words_compact, min_count=15, max_count=0,\n",
        "                     max_replacement=None, min_replacement=None):\n",
        "        \"\"\" Replace word indices below min_count with the pad index.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        words_compact: int array\n",
        "            Source array whose values will be replaced. This is assumed to\n",
        "            already be converted into a compact array with `to_compact`.\n",
        "        min_count : int\n",
        "            Replace words less frequently occuring than this count. This\n",
        "            defines the threshold for what words are very rare\n",
        "        max_count : int\n",
        "            Replace words occuring more frequently than this count. This\n",
        "            defines the threshold for very frequent words\n",
        "        min_replacement : int, default is out_of_vocabulary\n",
        "            Replace words less than min_count with this.\n",
        "        max_replacement : int, default is out_of_vocabulary\n",
        "            Replace words greater than max_count with this.\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> corpus = Corpus()\n",
        "        >>> # Make 1000 word indices with index < 100 and\n",
        "        >>> # update the word counts.\n",
        "        >>> word_indices = np.random.randint(100, size=1000)\n",
        "        >>> corpus.update_word_count(word_indices)\n",
        "        >>> corpus.finalize()  # any word indices above 99 will be filtered\n",
        "        >>> # Now create a new text, but with some indices above 100\n",
        "        >>> word_indices = np.random.randint(200, size=1000)\n",
        "        >>> word_indices.max() < 100\n",
        "        False\n",
        "        >>> # Remove words that have never appeared in the original corpus.\n",
        "        >>> filtered = corpus.filter_count(word_indices, min_count=1)\n",
        "        >>> filtered.max() < 100\n",
        "        True\n",
        "        >>> # We can also remove highly frequent words.\n",
        "        >>> filtered = corpus.filter_count(word_indices, max_count=2)\n",
        "        >>> len(np.unique(word_indices)) > len(np.unique(filtered))\n",
        "        True\n",
        "        \"\"\"\n",
        "        self._check_finalized()\n",
        "        ret = words_compact.copy()\n",
        "        if min_replacement is None:\n",
        "            min_replacement = self.specials_to_compact['out_of_vocabulary']\n",
        "        if max_replacement is None:\n",
        "            max_replacement = self.specials_to_compact['out_of_vocabulary']\n",
        "        not_specials = np.ones(self.keys_counts.shape[0], dtype='bool')\n",
        "        not_specials[:self.n_specials] = False\n",
        "        if min_count:\n",
        "            # Find first index with count less than min_count\n",
        "            min_idx = np.argmax(not_specials & (self.keys_counts < min_count))\n",
        "            # Replace all indices greater than min_idx\n",
        "            ret[ret > min_idx] = min_replacement\n",
        "        if max_count:\n",
        "            # Find first index with count less than max_count\n",
        "            max_idx = np.argmax(not_specials & (self.keys_counts < max_count))\n",
        "            # Replace all indices less than max_idx\n",
        "            ret[ret < max_idx] = max_replacement\n",
        "        return ret\n",
        "\n",
        "    def subsample_frequent(self, words_compact, threshold=1e-5):\n",
        "        \"\"\" Subsample the most frequent words. This aggressively\n",
        "        replaces words with frequencies higher than `threshold`. Words\n",
        "        are replaced with the out_of_vocabulary token.\n",
        "\n",
        "        Words will be replaced with probability as a function of their\n",
        "        frequency in the training corpus:\n",
        "\n",
        "        .. math::\n",
        "            p(w) = 1.0 - \\sqrt{threshold\\over f(w)}\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        words_compact: int array\n",
        "            The input array to subsample.\n",
        "        threshold: float in [0, 1]\n",
        "            Words with frequencies higher than this will be increasingly\n",
        "            subsampled.\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> corpus = Corpus()\n",
        "        >>> word_indices = (np.random.power(5.0, size=1000) * 100).astype('i')\n",
        "        >>> corpus.update_word_count(word_indices)\n",
        "        >>> corpus.finalize()\n",
        "        >>> compact = corpus.to_compact(word_indices)\n",
        "        >>> sampled = corpus.subsample_frequent(compact, threshold=1e-2)\n",
        "        >>> skip = corpus.specials_to_compact['skip']\n",
        "        >>> np.sum(compact == skip)  # No skips in the compact tokens\n",
        "        0\n",
        "        >>> np.sum(sampled == skip) > 0  # Many skips in the sampled tokens\n",
        "        True\n",
        "\n",
        "        .. [1] Distributed Representations of Words and Phrases and\n",
        "               their Compositionality. Mikolov, Tomas and Sutskever, Ilya\n",
        "               and Chen, Kai and Corrado, Greg S and Dean, Jeff\n",
        "               Advances in Neural Information Processing Systems 26\n",
        "        \"\"\"\n",
        "        self._check_finalized()\n",
        "        freq = self.keys_frequency + 1e-10\n",
        "        pw = 1.0 - (np.sqrt(threshold / freq) + threshold / freq)\n",
        "        prob = fast_replace(words_compact, self.keys_compact, pw)\n",
        "        draw = np.random.uniform(size=prob.shape)\n",
        "        ret = words_compact.copy()\n",
        "        # If probability greater than draw, skip the word\n",
        "        ret[prob > draw] = self.specials_to_compact['skip']\n",
        "        return ret\n",
        "\n",
        "    def to_compact(self, word_loose):\n",
        "        \"\"\" Convert a loose word index matrix to a compact array using\n",
        "        a fixed loose to dense mapping. Out of vocabulary word indices\n",
        "        will be replaced by the out of vocabulary index. The most common\n",
        "        index will be mapped to 0, the next most common to 1, and so on.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        word_loose : int array\n",
        "            Input loose word array to be converted into a compact array.\n",
        "\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> corpus = Corpus()\n",
        "        >>> word_indices = np.random.randint(100, size=1000)\n",
        "        >>> n_words = len(np.unique(word_indices))\n",
        "        >>> corpus.update_word_count(word_indices)\n",
        "        >>> corpus.finalize()\n",
        "        >>> word_compact = corpus.to_compact(word_indices)\n",
        "        >>> # The most common word in the training set will be mapped to be\n",
        "        >>> # right after all the special tokens, so 2 in this case.\n",
        "        >>> np.argmax(np.bincount(word_compact)) == 2\n",
        "        True\n",
        "        >>> most_common = np.argmax(np.bincount(word_indices))\n",
        "        >>> corpus.loose_to_compact[most_common] == 2\n",
        "        True\n",
        "        >>> # Out of vocabulary indices will be mapped to 1\n",
        "        >>> word_indices = np.random.randint(150, size=1000)\n",
        "        >>> word_compact_oov = corpus.to_compact(word_indices)\n",
        "        >>> oov = corpus.specials_to_compact['out_of_vocabulary']\n",
        "        >>> oov\n",
        "        1\n",
        "        >>> oov in word_compact\n",
        "        False\n",
        "        >>> oov in word_compact_oov\n",
        "        True\n",
        "        \"\"\"\n",
        "        self._check_finalized()\n",
        "        keys = self.keys_loose\n",
        "        reps = self.keys_compact\n",
        "        uniques = np.unique(word_loose)\n",
        "        # Find the out of vocab indices\n",
        "        oov = np.setdiff1d(uniques, keys, assume_unique=True)\n",
        "        oov_token = self.specials_to_compact['out_of_vocabulary']\n",
        "        keys = np.concatenate((keys, oov))\n",
        "        reps = np.concatenate((reps, np.zeros_like(oov) + oov_token))\n",
        "        compact = fast_replace(word_loose, keys, reps)\n",
        "        msg = \"Error: all compact indices should be non-negative\"\n",
        "        assert compact.min() >= 0, msg\n",
        "        return compact\n",
        "\n",
        "    def to_loose(self, word_compact):\n",
        "        \"\"\" Convert a compacted array back into a loose array.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        word_compact : int array\n",
        "            Input compacted word array to be converted into a loose array.\n",
        "\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> corpus = Corpus()\n",
        "        >>> word_indices = np.random.randint(100, size=1000)\n",
        "        >>> corpus.update_word_count(word_indices)\n",
        "        >>> corpus.finalize()\n",
        "        >>> word_compact = corpus.to_compact(word_indices)\n",
        "        >>> word_loose = corpus.to_loose(word_compact)\n",
        "        >>> np.all(word_loose == word_indices)\n",
        "        True\n",
        "        \"\"\"\n",
        "        self._check_finalized()\n",
        "        uniques = np.unique(word_compact)\n",
        "        # Find the out of vocab indices\n",
        "        oov = np.setdiff1d(uniques, self.keys_compact, assume_unique=True)\n",
        "        msg = \"Found keys in `word_compact` not present in the\"\n",
        "        msg += \"training corpus. Is this actually a compacted array?\"\n",
        "        assert np.all(oov < 0), msg\n",
        "        loose = fast_replace(word_compact, self.keys_compact, self.keys_loose)\n",
        "        return loose\n",
        "\n",
        "    def compact_to_flat(self, word_compact, *components):\n",
        "        \"\"\" Ravel a 2D compact array of documents (rows) and word\n",
        "        positions (columns) into a 1D array of words. Leave out special\n",
        "        tokens and ravel the component arrays in the same fashion.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        word_compact : int array\n",
        "            Array of word indices in documents. Has shape (n_docs, max_length)\n",
        "        components : list of arrays\n",
        "            A list of arrays detailing per-document properties. Each array\n",
        "            must n_docs long.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        flat : int array\n",
        "            An array of all words unravelled into a 1D shape\n",
        "        components : list of arrays\n",
        "            Each array here is also unravelled into the same shape\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> corpus = Corpus()\n",
        "        >>> word_indices = np.random.randint(100, size=1000)\n",
        "        >>> corpus.update_word_count(word_indices)\n",
        "        >>> corpus.finalize()\n",
        "        >>> doc_texts = np.arange(8).reshape((2, 4))\n",
        "        >>> doc_texts[:, -1] = -2  # Mark as skips\n",
        "        >>> doc_ids = np.arange(2)\n",
        "        >>> compact = corpus.to_compact(doc_texts)\n",
        "        >>> oov = corpus.specials_to_compact['out_of_vocabulary']\n",
        "        >>> compact[1, 3] = oov  # Mark the last word as OOV\n",
        "        >>> flat = corpus.compact_to_flat(compact)\n",
        "        >>> flat.shape[0] == 6  # 2 skips were dropped from 8 words\n",
        "        True\n",
        "        >>> flat[-1] == corpus.loose_to_compact[doc_texts[1, 2]]\n",
        "        True\n",
        "        >>> flat, (flat_id,) = corpus.compact_to_flat(compact, doc_ids)\n",
        "        >>> flat_id\n",
        "        array([0, 0, 0, 1, 1, 1])\n",
        "        \"\"\"\n",
        "        self._check_finalized()\n",
        "        n_docs = word_compact.shape[0]\n",
        "        max_length = word_compact.shape[1]\n",
        "        idx = word_compact > self.n_specials\n",
        "        components_raveled = []\n",
        "        msg = \"Length of each component must much `word_compact` size\"\n",
        "        for component in components:\n",
        "            raveled = np.tile(component[:, None], max_length)[idx]\n",
        "            components_raveled.append(raveled)\n",
        "            assert len(component) == n_docs, msg\n",
        "        if len(components_raveled) == 0:\n",
        "            return word_compact[idx]\n",
        "        else:\n",
        "            return word_compact[idx], components_raveled\n",
        "\n",
        "    def word_list(self, vocab, max_compact_index=None, oov_token='<OoV>'):\n",
        "        \"\"\" Translate compact keys back into string representations for a word.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        vocab : dict\n",
        "            The vocab object has loose indices as keys and word strings as\n",
        "            values.\n",
        "\n",
        "        max_compact_index : int\n",
        "            Only return words up to this index. If None, defaults to the number\n",
        "            of compact indices available\n",
        "\n",
        "        oov_token : str\n",
        "            Returns this string if a compact index does not have a word in the\n",
        "            vocab dictionary provided.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        word_list : list\n",
        "            A list of strings representations corresponding to word indices\n",
        "            zero to `max_compact_index`\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "\n",
        "        >>> vocab = {0: 'But', 1: 'the', 2: 'night', 3: 'was', 4: 'warm'}\n",
        "        >>> word_indices = np.zeros(50).astype('int32')\n",
        "        >>> word_indices[:25] = 0  # 'But' shows 25 times\n",
        "        >>> word_indices[25:35] = 1  # 'the' is in 10 times\n",
        "        >>> word_indices[40:46] = 2  # 'night' is in 6 times\n",
        "        >>> word_indices[46:49] = 3  # 'was' is in 3 times\n",
        "        >>> word_indices[49:] = 4  # 'warm' in in 2 times\n",
        "        >>> corpus = Corpus()\n",
        "        >>> corpus.update_word_count(word_indices)\n",
        "        >>> corpus.finalize()\n",
        "        >>> # Build a vocabulary of word indices\n",
        "        >>> corpus.word_list(vocab)\n",
        "        ['skip', 'out_of_vocabulary', 'But', 'the', 'night', 'was', 'warm']\n",
        "        \"\"\"\n",
        "        # Translate the compact keys into string words\n",
        "        oov = self.specials['out_of_vocabulary']\n",
        "        words = []\n",
        "        if max_compact_index is None:\n",
        "            max_compact_index = self.keys_compact.shape[0]\n",
        "        index_to_special = {i: s for s, i in self.specials.items()}\n",
        "        for compact_index in range(max_compact_index):\n",
        "            loose_index = self.compact_to_loose.get(compact_index, oov)\n",
        "            special = index_to_special.get(loose_index, oov_token)\n",
        "            string = vocab.get(loose_index, special)\n",
        "            words.append(string)\n",
        "        return words\n",
        "\n",
        "    def compact_word_vectors(self, vocab, filename=None, array=None,\n",
        "                             top=20000):\n",
        "        \"\"\" Retrieve pretrained word spectors for our vocabulary.\n",
        "        The returned word array has row indices corresponding to the\n",
        "        compact index of a word, and columns correponding to the word\n",
        "        vector.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        vocab : dict\n",
        "            Dictionary where keys are the loose index, and values are\n",
        "            the word string.\n",
        "\n",
        "        use_spacy : bool\n",
        "            Use SpaCy to load in word vectors. Otherwise Gensim.\n",
        "\n",
        "        filename : str\n",
        "            Filename for SpaCy-compatible word vectors or if use_spacy=False\n",
        "            then uses word2vec vectors via gensim.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        data : numpy float array\n",
        "            Array such that data[compact_index, :] = word_vector\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> import numpy.linalg as nl\n",
        "        >>> vocab = {19: 'shuttle', 5: 'astronomy', 7: 'cold', 3: 'hot'}\n",
        "        >>> word_indices = np.zeros(50).astype('int32')\n",
        "        >>> word_indices[:25] = 19  # 'Shuttle' shows 25 times\n",
        "        >>> word_indices[25:35] = 5  # 'astronomy' is in 10 times\n",
        "        >>> word_indices[40:46] = 7  # 'cold' is in 6 times\n",
        "        >>> word_indices[46:] = 3  # 'hot' is in 3 times\n",
        "        >>> corpus = Corpus()\n",
        "        >>> corpus.update_word_count(word_indices)\n",
        "        >>> corpus.finalize()\n",
        "        >>> v, s, f = corpus.compact_word_vectors(vocab)\n",
        "        >>> sim = lambda x, y: np.dot(x, y) / nl.norm(x) / nl.norm(y)\n",
        "        >>> vocab[corpus.compact_to_loose[2]]\n",
        "        'shuttle'\n",
        "        >>> vocab[corpus.compact_to_loose[3]]\n",
        "        'astronomy'\n",
        "        >>> vocab[corpus.compact_to_loose[4]]\n",
        "        'cold'\n",
        "        >>> sim_shuttle_astro = sim(v[2, :], v[3, :])\n",
        "        >>> sim_shuttle_cold = sim(v[2, :], v[4, :])\n",
        "        >>> sim_shuttle_astro > sim_shuttle_cold\n",
        "        True\n",
        "        \"\"\"\n",
        "        n_words = len(self.compact_to_loose)\n",
        "        from gensim.models.word2vec import Word2Vec\n",
        "        model = Word2Vec.load_word2vec_format(filename, binary=True)\n",
        "        n_dim = model.syn0.shape[1]\n",
        "        data = np.random.normal(size=(n_words, n_dim)).astype('float32')\n",
        "        data -= data.mean()\n",
        "        data += model.syn0.mean()\n",
        "        data /= data.std()\n",
        "        data *= model.syn0.std()\n",
        "        if array is not None:\n",
        "            data = array\n",
        "            n_words = data.shape[0]\n",
        "        keys_raw = model.vocab.keys()\n",
        "        keys = [s.encode('ascii', 'ignore') for s in keys_raw]\n",
        "        lens = [len(s) for s in model.vocab.keys()]\n",
        "        choices = np.array(keys, dtype='S')\n",
        "        lengths = np.array(lens, dtype='int32')\n",
        "        s, f = 0, 0\n",
        "        rep0 = lambda w: w\n",
        "        rep1 = lambda w: w.replace(' ', '_')\n",
        "        rep2 = lambda w: w.title().replace(' ', '_')\n",
        "        reps = [rep0, rep1, rep2]\n",
        "        for compact in np.arange(top):\n",
        "            loose = self.compact_to_loose.get(compact, None)\n",
        "            if loose is None:\n",
        "                continue\n",
        "            word = vocab.get(loose, None)\n",
        "            if word is None:\n",
        "                continue\n",
        "            word = word.strip()\n",
        "            vector = None\n",
        "            for rep in reps:\n",
        "                clean = rep(word)\n",
        "                if clean in model.vocab:\n",
        "                    vector = model[clean]\n",
        "                    break\n",
        "            if vector is None:\n",
        "                try:\n",
        "                    word = unicode(word)\n",
        "                    idx = lengths >= len(word) - 3\n",
        "                    idx &= lengths <= len(word) + 3\n",
        "                    sel = choices[idx]\n",
        "                    d = damerau_levenshtein_distance_withNPArray(word, sel)\n",
        "                    choice = np.array(keys_raw)[idx][np.argmin(d)]\n",
        "                    # choice = difflib.get_close_matches(word, choices)[0]\n",
        "                    vector = model[choice]\n",
        "                    print(compact, word, ' --> ', choice)\n",
        "                except IndexError:\n",
        "                    pass\n",
        "            if vector is None:\n",
        "                f += 1\n",
        "                continue\n",
        "            s += 1\n",
        "            data[compact, :] = vector[:]\n",
        "        return data, s, f\n",
        "\n",
        "    def compact_to_bow(self, word_compact, max_compact_index=None):\n",
        "        \"\"\" Given a 2D array of compact indices, return the bag of words\n",
        "        representation where the column is the word index, row is the document\n",
        "        index, and the value is the number of times that word appears in that\n",
        "        document.\n",
        "\n",
        "        >>> import numpy.linalg as nl\n",
        "        >>> vocab = {19: 'shuttle', 5: 'astronomy', 7: 'cold', 3: 'hot'}\n",
        "        >>> word_indices = np.zeros(50).astype('int32')\n",
        "        >>> word_indices[:25] = 19  # 'Shuttle' shows 25 times\n",
        "        >>> word_indices[25:35] = 5  # 'astronomy' is in 10 times\n",
        "        >>> word_indices[40:46] = 7  # 'cold' is in 6 times\n",
        "        >>> word_indices[46:] = 3  # 'hot' is in 3 times\n",
        "        >>> corpus = Corpus()\n",
        "        >>> corpus.update_word_count(word_indices)\n",
        "        >>> corpus.finalize()\n",
        "        >>> v = corpus.compact_to_bow(word_indices)\n",
        "        >>> len(v)\n",
        "        20\n",
        "        >>> v[:6]\n",
        "        array([ 5,  0,  0,  4,  0, 10])\n",
        "        >>> v[19]\n",
        "        25\n",
        "        >>> v.sum()\n",
        "        50\n",
        "        >>> words = [[0, 0, 0, 3, 4], [1, 1, 1, 4, 5]]\n",
        "        >>> words = np.array(words)\n",
        "        >>> bow = corpus.compact_to_bow(words)\n",
        "        >>> bow.shape\n",
        "        (2, 6)\n",
        "        \"\"\"\n",
        "        if max_compact_index is None:\n",
        "            max_compact_index = word_compact.max()\n",
        "\n",
        "        def bincount(x):\n",
        "            return np.bincount(x, minlength=max_compact_index + 1)\n",
        "        axis = len(word_compact.shape) - 1\n",
        "        bow = np.apply_along_axis(bincount, axis, word_compact)\n",
        "        return bow\n",
        "\n",
        "    def compact_to_coocurrence(self, word_compact, indices, window_size=10):\n",
        "        \"\"\" From an array of compact tokens and aligned array of document indices\n",
        "        compute (word, word, document) co-occurrences within a moving window.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        word_compact: int array\n",
        "        Sequence of tokens.\n",
        "\n",
        "        indices: dict of int arrays\n",
        "        Each array in this dictionary should represent the document index it\n",
        "        came from.\n",
        "\n",
        "        window_size: int\n",
        "        Indicates the moving window size around which all co-occurrences will\n",
        "        be computed.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        counts : DataFrame\n",
        "        Returns a DataFrame with two columns for word index A and B,\n",
        "        one extra column for each document index, and a final column for counts\n",
        "        in that key.\n",
        "\n",
        "        >>> compact = np.array([0, 1, 1, 1, 2, 2, 3, 0])\n",
        "        >>> doc_idx = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
        "        >>> corpus = Corpus()\n",
        "        >>> counts = corpus.compact_to_coocurrence(compact, {'doc': doc_idx})\n",
        "        >>> counts.counts.sum()\n",
        "        24\n",
        "        >>> counts.query('doc == 0').counts.values\n",
        "        array([3, 3, 6])\n",
        "        >>> compact = np.array([0, 1, 1, 1, 2, 2, 3, 0])\n",
        "        >>> doc_idx = np.array([0, 0, 0, 1, 1, 2, 2, 2])\n",
        "        >>> corpus = Corpus()\n",
        "        >>> counts = corpus.compact_to_coocurrence(compact, {'doc': doc_idx})\n",
        "        >>> counts.counts.sum()\n",
        "        14\n",
        "        >>> counts.query('doc == 0').word_index_x.values\n",
        "        array([0, 1, 1])\n",
        "        >>> counts.query('doc == 0').word_index_y.values\n",
        "        array([1, 0, 1])\n",
        "        >>> counts.query('doc == 0').counts.values\n",
        "        array([2, 2, 2])\n",
        "        >>> counts.query('doc == 1').counts.values\n",
        "        array([1, 1])\n",
        "        \"\"\"\n",
        "        tokens = pd.DataFrame(dict(word_index=word_compact)).reset_index()\n",
        "        for name, index in indices.items():\n",
        "            tokens[name] = index\n",
        "        a, b = tokens.copy(), tokens.copy()\n",
        "        mask = lambda x: np.prod([x[k + '_x'] == x[k + '_y']\n",
        "                                  for k in indices.keys()], axis=0)\n",
        "        group_keys = ['word_index_x', 'word_index_y', ]\n",
        "        group_keys += [k + '_x' for k in indices.keys()]\n",
        "        total = []\n",
        "        a['frame'] = a['index'].copy()\n",
        "        for frame in range(-window_size, window_size + 1):\n",
        "            if frame == 0:\n",
        "                continue\n",
        "            b['frame'] = b['index'] + frame\n",
        "            matches = (a.merge(b, on='frame')\n",
        "                        .assign(same_doc=mask)\n",
        "                        .pipe(lambda df: df[df['same_doc'] == 1])\n",
        "                        .groupby(group_keys)['frame']\n",
        "                        .count()\n",
        "                        .reset_index())\n",
        "            total.append(matches)\n",
        "        counts = (pd.concat(total)\n",
        "                    .groupby(group_keys)['frame']\n",
        "                    .sum()\n",
        "                    .reset_index()\n",
        "                    .rename(columns={k + '_x': k for k in indices.keys()})\n",
        "                    .rename(columns=dict(frame='counts')))\n",
        "        return counts\n",
        "\n",
        "\n",
        "def fast_replace(data, keys, values, skip_checks=False):\n",
        "    \"\"\" Do a search-and-replace in array `data`.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    data : int array\n",
        "        Array of integers\n",
        "    keys : int array\n",
        "        Array of keys inside of `data` to be replaced\n",
        "    values : int array\n",
        "        Array of values that replace the `keys` array\n",
        "    skip_checks : bool, default=False\n",
        "        Optionally skip sanity checking the input.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> fast_replace(np.arange(5), np.arange(5), np.arange(5)[::-1])\n",
        "    array([4, 3, 2, 1, 0])\n",
        "    \"\"\"\n",
        "    assert np.allclose(keys.shape, values.shape)\n",
        "    if not skip_checks:\n",
        "        msg = \"data has elements not in keys\"\n",
        "        assert data.max() <= keys.max(), msg\n",
        "    sdx = np.argsort(keys)\n",
        "    keys, values = keys[sdx], values[sdx]\n",
        "    idx = np.digitize(data, keys, right=True)\n",
        "    new_data = values[idx]\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nahcS1zA48uv"
      },
      "outputs": [],
      "source": [
        "!pip install chainer\n",
        "import chainer.functions as F\n",
        "from chainer import Variable\n",
        "\n",
        "\n",
        "def dirichlet_likelihood(weights, alpha=None):\n",
        "    \"\"\" Calculate the log likelihood of the observed topic proportions.\n",
        "    A negative likelihood is more likely than a negative likelihood.\n",
        "\n",
        "    Args:\n",
        "        weights (chainer.Variable): Unnormalized weight vector. The vector\n",
        "            will be passed through a softmax function that will map the input\n",
        "            onto a probability simplex.\n",
        "        alpha (float): The Dirichlet concentration parameter. Alpha\n",
        "            greater than 1.0 results in very dense topic weights such\n",
        "            that each document belongs to many topics. Alpha < 1.0 results\n",
        "            in sparser topic weights. The default is to set alpha to\n",
        "            1.0 / n_topics, effectively enforcing the prior belief that a\n",
        "            document belong to very topics at once.\n",
        "\n",
        "    Returns:\n",
        "        ~chainer.Variable: Output loss variable.\n",
        "    \"\"\"\n",
        "    if type(weights) is Variable:\n",
        "        n_topics = weights.data.shape[1]\n",
        "    else:\n",
        "        n_topics = weights.W.data.shape[1]\n",
        "    if alpha is None:\n",
        "        alpha = 1.0 / n_topics\n",
        "    if type(weights) is Variable:\n",
        "        log_proportions = F.log_softmax(weights)\n",
        "    else:\n",
        "        log_proportions = F.log_softmax(weights.W)\n",
        "    loss = (alpha - 1.0) * log_proportions\n",
        "    return -F.sum(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARWkUUv252KK"
      },
      "outputs": [],
      "source": [
        "import chainer\n",
        "import chainer.links as L\n",
        "\n",
        "\n",
        "def _orthogonal_matrix(shape):\n",
        "    # Stolen from blocks:\n",
        "    # github.com/mila-udem/blocks/blob/master/blocks/initialization.py\n",
        "    M1 = np.random.randn(shape[0], shape[0])\n",
        "    M2 = np.random.randn(shape[1], shape[1])\n",
        "\n",
        "    # QR decomposition of matrix with entries in N(0, 1) is random\n",
        "    Q1, R1 = np.linalg.qr(M1)\n",
        "    Q2, R2 = np.linalg.qr(M2)\n",
        "    # Correct that NumPy doesn't force diagonal of R to be non-negative\n",
        "    Q1 = Q1 * np.sign(np.diag(R1))\n",
        "    Q2 = Q2 * np.sign(np.diag(R2))\n",
        "\n",
        "    n_min = min(shape[0], shape[1])\n",
        "    return np.dot(Q1[:, :n_min], Q2[:n_min, :])\n",
        "\n",
        "\n",
        "class EmbedMixture(chainer.Chain):\n",
        "    \"\"\" A single document is encoded as a multinomial mixture of latent topics.\n",
        "    The mixture is defined on simplex, so that mixture weights always sum\n",
        "    to 100%. The latent topic vectors resemble word vectors whose elements are\n",
        "    defined over all real numbers.\n",
        "\n",
        "    For example, a single document mix may be :math:`[0.9, 0.1]`, indicating\n",
        "    that it is 90% in the first topic, 10% in the second. An example topic\n",
        "    vector looks like :math:`[1.5e1, -1.3e0, +3.4e0, -0.2e0]`, which is\n",
        "    largely uninterpretable until you measure the words most similar to this\n",
        "    topic vector.\n",
        "\n",
        "    A single document vector :math:`\\vec{e}` is composed as weights :math:`c_j`\n",
        "    over topic vectors :math:`\\vec{T_j}`:\n",
        "\n",
        "    .. math::\n",
        "\n",
        "        \\vec{e}=\\Sigma_{j=0}^{j=n\\_topics}c_j\\vec{T_j}\n",
        "\n",
        "    This is usually paired with regularization on the weights :math:`c_j`.\n",
        "    If using a Dirichlet prior with low alpha, these weights will be sparse.\n",
        "\n",
        "    Args:\n",
        "        n_documents (int): Total number of documents\n",
        "        n_topics (int): Number of topics per document\n",
        "        n_dim (int): Number of dimensions per topic vector (should match word\n",
        "            vector size)\n",
        "\n",
        "    Attributes:\n",
        "        weights : chainer.links.EmbedID\n",
        "            Unnormalized topic weights (:math:`c_j`). To normalize these\n",
        "            weights, use `F.softmax(weights)`.\n",
        "        factors : chainer.links.Parameter\n",
        "            Topic vector matrix (:math:`T_j`)\n",
        "\n",
        "    .. seealso:: :func:`lda2vec.dirichlet_likelihood`\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_documents, n_topics, n_dim, dropout_ratio=0.2,\n",
        "                 temperature=1.0):\n",
        "        self.n_documents = n_documents\n",
        "        self.n_topics = n_topics\n",
        "        self.n_dim = n_dim\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        factors = _orthogonal_matrix((n_topics, n_dim)).astype('float32')\n",
        "        factors /= np.sqrt(n_topics + n_dim)\n",
        "        super(EmbedMixture, self).__init__(\n",
        "            weights=L.EmbedID(n_documents, n_topics),\n",
        "            factors=L.Parameter(factors))\n",
        "        self.temperature = temperature\n",
        "        self.weights.W.data[...] /= np.sqrt(n_documents + n_topics)\n",
        "\n",
        "    def __call__(self, doc_ids, update_only_docs=False):\n",
        "        \"\"\" Given an array of document integer indices, returns a vector\n",
        "        for each document. The vector is composed of topic weights projected\n",
        "        onto topic vectors.\n",
        "\n",
        "        Args:\n",
        "            doc_ids : chainer.Variable\n",
        "                One-dimensional batch vectors of IDs\n",
        "\n",
        "        Returns:\n",
        "            doc_vector : chainer.Variable\n",
        "                Batch of two-dimensional embeddings for every document.\n",
        "        \"\"\"\n",
        "        # (batchsize, ) --> (batchsize, multinomial)\n",
        "        proportions = self.proportions(doc_ids, softmax=True)\n",
        "        # (batchsize, n_factors) * (n_factors, n_dim) --> (batchsize, n_dim)\n",
        "        factors = F.dropout(self.factors(), ratio=self.dropout_ratio)\n",
        "        if update_only_docs:\n",
        "            factors.unchain_backward()\n",
        "        w_sum = F.matmul(proportions, factors)\n",
        "        return w_sum\n",
        "\n",
        "    def proportions(self, doc_ids, softmax=False):\n",
        "        \"\"\" Given an array of document indices, return a vector\n",
        "        for each document of just the unnormalized topic weights.\n",
        "\n",
        "        Returns:\n",
        "            doc_weights : chainer.Variable\n",
        "                Two dimensional topic weights of each document.\n",
        "        \"\"\"\n",
        "        w = self.weights(doc_ids)\n",
        "        if softmax:\n",
        "            size = w.data.shape\n",
        "            mask = self.xp.random.random_integers(0, 1, size=size)\n",
        "            y = (F.softmax(w * self.temperature) *\n",
        "                 Variable(mask.astype('float32')))\n",
        "            norm, y = F.broadcast(F.expand_dims(F.sum(y, axis=1), 1), y)\n",
        "            return y / (norm + 1e-7)\n",
        "        else:\n",
        "            return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NggbWkC9-vn1"
      },
      "outputs": [],
      "source": [
        "from numpy.random import random_sample\n",
        "\n",
        "\n",
        "def orthogonal_matrix(shape):\n",
        "    # Stolen from blocks:\n",
        "    # github.com/mila-udem/blocks/blob/master/blocks/initialization.py\n",
        "    M1 = np.random.randn(shape[0], shape[0])\n",
        "    M2 = np.random.randn(shape[1], shape[1])\n",
        "\n",
        "    # QR decomposition of matrix with entries in N(0, 1) is random\n",
        "    Q1, R1 = np.linalg.qr(M1)\n",
        "    Q2, R2 = np.linalg.qr(M2)\n",
        "    # Correct that NumPy doesn't force diagonal of R to be non-negative\n",
        "    Q1 = Q1 * np.sign(np.diag(R1))\n",
        "    Q2 = Q2 * np.sign(np.diag(R2))\n",
        "\n",
        "    n_min = min(shape[0], shape[1])\n",
        "    return np.dot(Q1[:, :n_min], Q2[:n_min, :])\n",
        "\n",
        "\n",
        "def softmax(w):\n",
        "    # https://gist.github.com/stober/1946926\n",
        "    w = np.array(w)\n",
        "    maxes = np.amax(w, axis=1)\n",
        "    maxes = maxes.reshape(maxes.shape[0], 1)\n",
        "    e = np.exp(w - maxes)\n",
        "    dist = e / np.sum(e, axis=1)[:, None]\n",
        "    return dist\n",
        "\n",
        "\n",
        "def sample(values, probabilities, size):\n",
        "    assert np.allclose(np.sum(probabilities, axis=-1), 1.0)\n",
        "    bins = np.add.accumulate(probabilities)\n",
        "    return values[np.digitize(random_sample(size), bins)]\n",
        "\n",
        "\n",
        "def fake_data(n_docs, n_words, n_sent_length, n_topics):\n",
        "    \"\"\" Generate latent topic vectors for words and documents\n",
        "    and then for each document, draw a sentence. Draw each word\n",
        "    document with probability proportional to the dot product and\n",
        "    normalized with a softmax.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    n_docs : int\n",
        "        Number of documents\n",
        "    n_words : int\n",
        "        Number of words in the vocabulary\n",
        "    n_sent_length : int\n",
        "        Number of words to draw for each document\n",
        "    n_topics : int\n",
        "        Number of topics that a single document can belong to.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    sentences : int array\n",
        "        Array of word indices of shape (n_docs, n_sent_length).\n",
        "\n",
        "    \"\"\"\n",
        "    # These are log ratios for the doc & word topics\n",
        "    doc_topics = orthogonal_matrix([n_docs, n_topics])\n",
        "    wrd_topics = orthogonal_matrix([n_topics, n_words])\n",
        "    # Multiply log ratios and softmax to get prob of word in doc\n",
        "    doc_to_wrds = softmax(np.dot(doc_topics, wrd_topics))\n",
        "    # Now sample from doc_to_wrd to get realizations\n",
        "    indices = np.arange(n_words).astype('int32')\n",
        "    sentences = []\n",
        "    for doc_to_wrd in doc_to_wrds:\n",
        "        words = sample(indices, doc_to_wrd, n_sent_length)\n",
        "        sentences.append(words)\n",
        "    sentences = np.array(sentences)\n",
        "    return sentences.astype('int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftWXEhCU-1vv"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import six\n",
        "\n",
        "from chainer import cuda\n",
        "from chainer.utils import type_check\n",
        "\n",
        "\n",
        "class NegativeSamplingFunction(function.Function):\n",
        "\n",
        "    ignore_label = -1\n",
        "\n",
        "    def __init__(self, sampler, sample_size):\n",
        "        self.sampler = sampler\n",
        "        self.sample_size = sample_size\n",
        "\n",
        "    def _make_samples(self, t):\n",
        "        if hasattr(self, 'samples'):\n",
        "            return self.samples  # for testing\n",
        "\n",
        "        size = int(t.shape[0])\n",
        "        # first one is the positive, and others are sampled negatives\n",
        "        samples = self.sampler((size, self.sample_size + 1))\n",
        "        samples[:, 0] = t\n",
        "        self.samples = samples\n",
        "\n",
        "    def check_type_forward(self, in_types):\n",
        "        type_check.expect(in_types.size() == 3)\n",
        "        x_type, t_type, w_type = in_types\n",
        "\n",
        "        type_check.expect(\n",
        "            x_type.dtype == numpy.float32,\n",
        "            x_type.ndim == 2,\n",
        "            t_type.dtype == numpy.int32,\n",
        "            t_type.ndim == 1,\n",
        "            x_type.shape[0] == t_type.shape[0],\n",
        "            w_type.dtype == numpy.float32,\n",
        "            w_type.ndim == 2,\n",
        "        )\n",
        "\n",
        "    def forward_cpu(self, inputs):\n",
        "        x, t, W = inputs\n",
        "        self.ignore_mask = (t != self.ignore_label)\n",
        "        self._make_samples(t)\n",
        "\n",
        "        loss = numpy.float32(0.0)\n",
        "        for i, (ix, k) in enumerate(six.moves.zip(x[self.ignore_mask],\n",
        "                                    self.samples[self.ignore_mask])):\n",
        "            w = W[k]\n",
        "            f = w.dot(ix)\n",
        "            f[0] *= -1  # positive sample\n",
        "            loss += numpy.sum(numpy.logaddexp(f, 0))\n",
        "        return numpy.array(loss, numpy.float32),\n",
        "\n",
        "    def forward_gpu(self, inputs):\n",
        "        x, t, W = inputs\n",
        "        self.ignore_mask = (t != self.ignore_label)\n",
        "        n_in = x.shape[1]\n",
        "        self._make_samples(t)\n",
        "\n",
        "        self.wx = cuda.elementwise(\n",
        "            'raw T W, raw T x, bool mask, S k, int32 c, int32 m', 'T wx',\n",
        "            '''\n",
        "            T f = 0;\n",
        "            if (mask == 1){\n",
        "                for (int j = 0; j < c; ++j) {\n",
        "                  int x_ind[] = {(i / m), j};\n",
        "                  int w_ind[] = {k, j};\n",
        "                  f += x[x_ind] * W[w_ind];\n",
        "                }\n",
        "            }\n",
        "            wx = f;\n",
        "            ''',\n",
        "            'negative_sampling_wx'\n",
        "            )(W, x, self.ignore_mask[:, None], self.samples, n_in,\n",
        "              self.sample_size + 1)\n",
        "\n",
        "        y = cuda.elementwise(\n",
        "            'T wx, int32 c, int32 m', 'T y',\n",
        "            '''\n",
        "            T f = wx;\n",
        "            if (i % m == 0) {\n",
        "              f = -f;\n",
        "            }\n",
        "            T loss;\n",
        "            if (f < 0) {\n",
        "              loss = __logf(1 + __expf(f));\n",
        "            } else {\n",
        "              loss = f + __logf(1 + __expf(-f));\n",
        "            }\n",
        "            y = loss;\n",
        "            ''',\n",
        "            'negative_sampling_forward'\n",
        "        )(self.wx, n_in, self.sample_size + 1)\n",
        "        # TODO(okuta): merge elementwise\n",
        "        loss = cuda.cupy.sum(y * self.ignore_mask[:, None].astype('float32'))\n",
        "        return loss,\n",
        "\n",
        "    def backward_cpu(self, inputs, grads):\n",
        "        x, t, W = inputs\n",
        "        gloss, = grads\n",
        "\n",
        "        gx = numpy.zeros_like(x)\n",
        "        gW = numpy.zeros_like(W)\n",
        "        for i, (ix, k) in enumerate(six.moves.zip(x[self.ignore_mask],\n",
        "                                    self.samples[self.ignore_mask])):\n",
        "            w = W[k]\n",
        "            f = w.dot(ix)\n",
        "\n",
        "            # g == -y * gloss / (1 + exp(yf))\n",
        "            f[0] *= -1\n",
        "            g = gloss / (1 + numpy.exp(-f))\n",
        "            g[0] *= -1\n",
        "\n",
        "            gx[i] = g.dot(w)\n",
        "            for ik, ig in six.moves.zip(k, g):\n",
        "                gW[ik] += ig * ix\n",
        "        return gx, None, gW\n",
        "\n",
        "    def backward_gpu(self, inputs, grads):\n",
        "        cupy = cuda.cupy\n",
        "        x, t, W = inputs\n",
        "        gloss, = grads\n",
        "\n",
        "        n_in = x.shape[1]\n",
        "        g = cuda.elementwise(\n",
        "            'T wx, raw T gloss, int32 m', 'T g',\n",
        "            '''\n",
        "            T y;\n",
        "            if (i % m == 0) {\n",
        "              y = 1;\n",
        "            } else {\n",
        "              y = -1;\n",
        "            }\n",
        "\n",
        "            g = -y * gloss[0] / (1.0f + __expf(wx * y));\n",
        "            ''',\n",
        "            'negative_sampling_calculate_g'\n",
        "        )(self.wx, gloss, self.sample_size + 1)\n",
        "        gx = cupy.zeros_like(x)\n",
        "        cuda.elementwise(\n",
        "            'raw T g, raw T W, bool mask, raw S k, int32 c, int32 m', 'T gx',\n",
        "            '''\n",
        "            int d = i / c;\n",
        "            T w = 0;\n",
        "            if (mask == 1){\n",
        "                for (int j = 0; j < m; ++j) {\n",
        "                  w += g[d * m + j] * W[k[d * m + j] * c + i % c];\n",
        "                }\n",
        "            }\n",
        "            gx = w;\n",
        "            ''',\n",
        "            'negative_sampling_calculate_gx'\n",
        "            )(g, W, self.ignore_mask[:, None], self.samples, n_in,\n",
        "              self.sample_size + 1, gx)\n",
        "        gW = cupy.zeros_like(W)\n",
        "        cuda.elementwise(\n",
        "            'T g, raw T x, S k, bool mask, int32 c, int32 m',\n",
        "            'raw T gW',\n",
        "            '''\n",
        "            T gi = g;\n",
        "            if (mask == 1) {\n",
        "                for (int j = 0; j < c; ++j) {\n",
        "                  atomicAdd(&gW[k * c + j], gi * x[(i / m) * c + j]);\n",
        "                }\n",
        "            }\n",
        "            ''',\n",
        "            'negative_sampling_calculate_gw'\n",
        "            )(g, x, self.samples, self.ignore_mask[:, None], n_in,\n",
        "              self.sample_size + 1, gW)\n",
        "        return gx, None, gW\n",
        "\n",
        "\n",
        "def negative_sampling(x, t, W, sampler, sample_size):\n",
        "    \"\"\"Negative sampling loss function.\n",
        "\n",
        "    In natural language processing, especially language modeling, the number of\n",
        "    words in a vocabulary can be very large.\n",
        "    Therefore, you need to spend a lot of time calculating the gradient of the\n",
        "    embedding matrix.\n",
        "\n",
        "    By using the negative sampling trick you only need to calculate the\n",
        "    gradient for a few sampled negative examples.\n",
        "\n",
        "    The objective function is below:\n",
        "\n",
        "    .. math::\n",
        "\n",
        "       f(x, p) = \\\\log \\\\sigma(x^\\\\top w_p) + \\\\\n",
        "       k E_{i \\\\sim P(i)}[\\\\log \\\\sigma(- x^\\\\top w_i)],\n",
        "\n",
        "    where :math:`\\sigma(\\cdot)` is a sigmoid function, :math:`w_i` is the\n",
        "    weight vector for the word :math:`i`, and :math:`p` is a positive example.\n",
        "    It is approximeted with :math:`k` examples :math:`N` sampled from\n",
        "    probability :math:`P(i)`, like this:\n",
        "\n",
        "    .. math::\n",
        "\n",
        "       f(x, p) \\\\approx \\\\log \\\\sigma(x^\\\\top w_p) + \\\\\n",
        "       \\\\sum_{n \\\\in N} \\\\log \\\\sigma(-x^\\\\top w_n).\n",
        "\n",
        "    Each sample of :math:`N` is drawn from the word distribution :math:`P(w)`.\n",
        "    This is calculated as :math:`P(w) = \\\\frac{1}{Z} c(w)^\\\\alpha`, where\n",
        "    :math:`c(w)` is the unigram count of the word :math:`w`, :math:`\\\\alpha` is\n",
        "    a hyper-parameter, and :math:`Z` is the normalization constant.\n",
        "\n",
        "    Args:\n",
        "        x (~chainer.Variable): Batch of input vectors.\n",
        "        t (~chainer.Variable): Vector of groundtruth labels.\n",
        "        W (~chainer.Variable): Weight matrix.\n",
        "        sampler (function): Sampling function. It takes a shape and returns an\n",
        "            integer array of the shape. Each element of this array is a sample\n",
        "            from the word distribution. A :class:`~chainer.utils.WalkerAlias`\n",
        "            object built with the power distribution of word frequency is\n",
        "            recommended.\n",
        "        sample_size (int): Number of samples.\n",
        "\n",
        "    See: `Distributed Representations of Words and Phrases and their\\\n",
        "         Compositionality <http://arxiv.org/abs/1310.4546>`_\n",
        "\n",
        "    .. seealso:: :class:`~chainer.links.NegativeSampling`.\n",
        "\n",
        "    \"\"\"\n",
        "    return NegativeSamplingFunction(sampler, sample_size)(x, t, W)\n",
        "\n",
        "\n",
        "# Monkey-patch the chainer code to replace the negative sampling\n",
        "# with the one used here\n",
        "import chainer.links as L\n",
        "import chainer.functions as F\n",
        "negative_sampling.patched = True\n",
        "L.NegativeSampling.negative_sampling = negative_sampling\n",
        "F.negative_sampling = negative_sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X72ozSd9_Ai0"
      },
      "outputs": [],
      "source": [
        "# !pip install spacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.attrs import LOWER, LIKE_URL, LIKE_EMAIL\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def tokenize(texts, max_length, skip=-2, attr='idx', merge=False, nlp=None,\n",
        "             **kwargs):\n",
        "    \"\"\" Uses spaCy to quickly tokenize text and return an array\n",
        "    of indices.\n",
        "\n",
        "    This method stores a global NLP directory in memory, and takes\n",
        "    up to a minute to run for the time. Later calls will have the\n",
        "    tokenizer in memory.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : list of unicode strings\n",
        "        These are the input documents. There can be multiple sentences per\n",
        "        item in the list.\n",
        "    max_length : int\n",
        "        This is the maximum number of words per document. If the document is\n",
        "        shorter then this number it will be padded to this length.\n",
        "    skip : int, optional\n",
        "        Short documents will be padded with this variable up until max_length.\n",
        "    attr : int, from spacy.attrs\n",
        "        What to transform the token to. Choice must be in spacy.attrs, and =\n",
        "        common choices are (LOWER, LEMMA)\n",
        "    merge : int, optional\n",
        "        Merge noun phrases into a single token. Useful for turning 'New York'\n",
        "        into a single token.\n",
        "    nlp : None\n",
        "        A spaCy NLP object. Useful for not reinstantiating the object multiple\n",
        "        times.\n",
        "    kwargs : dict, optional\n",
        "        Any further argument will be sent to the spaCy tokenizer. For extra\n",
        "        speed consider setting tag=False, parse=False, entity=False, or\n",
        "        n_threads=8.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    arr : 2D array of ints\n",
        "        Has shape (len(texts), max_length). Each value represents\n",
        "        the word index.\n",
        "    vocab : dict\n",
        "        Keys are the word index, and values are the string. The pad index gets\n",
        "        mapped to None\n",
        "    \"\"\"\n",
        "    if nlp is None:\n",
        "        nlp = English()\n",
        "    data = np.zeros((len(texts), max_length), dtype='int32')\n",
        "    data[:] = skip\n",
        "    bad_deps = ('amod', 'compound')\n",
        "    for row, doc in enumerate(nlp.pipe([text.lower() for text in texts], **kwargs)):\n",
        "        if merge:\n",
        "            # from the spaCy blog, an example on how to merge\n",
        "            # noun phrases into single tokens\n",
        "            for phrase in doc.noun_chunks:\n",
        "                # Only keep adjectives and nouns, e.g. \"good ideas\"\n",
        "                while len(phrase) > 1 and phrase[0].dep_ not in bad_deps:\n",
        "                    phrase = phrase[1:]\n",
        "                if len(phrase) > 1:\n",
        "                    # Merge the tokens, e.g. good_ideas\n",
        "                    phrase.merge(phrase.root.tag_, phrase.text,\n",
        "                                 phrase.root.ent_type_)\n",
        "                # Iterate over named entities\n",
        "                for ent in doc.ents:\n",
        "                    if len(ent) > 1:\n",
        "                        # Merge them into single tokens\n",
        "                        ent.merge(ent.root.tag_, ent.text, ent.label_)\n",
        "        dat = doc.to_array([attr, LIKE_EMAIL, LIKE_URL]).astype('int32')\n",
        "        if len(dat) > 0:\n",
        "            dat = dat.astype('int32')\n",
        "            msg = \"Negative indices reserved for special tokens\"\n",
        "            assert dat.min() >= 0, msg\n",
        "            # Replace email and URL tokens\n",
        "            idx = (dat[:, 1] > 0) | (dat[:, 2] > 0)\n",
        "            dat[idx] = skip\n",
        "            length = min(len(dat), max_length)\n",
        "            data[row, :length] = dat[:length, 0].ravel()\n",
        "    uniques = np.unique(data)\n",
        "    vocab = {v: nlp.vocab[v].lower_ for v in uniques if v != skip}\n",
        "    vocab[skip] = '<SKIP>'\n",
        "    return data, vocab\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     import doctest\n",
        "#     doctest.testmod()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxcqVvvTAFfB"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import multiprocessing\n",
        "\n",
        "def _softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    out = e_x / e_x.sum()\n",
        "    return out\n",
        "\n",
        "def _softmax_2d(x):\n",
        "    y = x - x.max(axis=1, keepdims=True)\n",
        "    np.exp(y, out=y)\n",
        "    y /= y.sum(axis=1, keepdims=True)\n",
        "    return y\n",
        "\n",
        "def prob_words(context, vocab, temperature=1.0):\n",
        "    \"\"\" This calculates a softmax over the vocabulary as a function\n",
        "    of the dot product of context and word.\n",
        "    \"\"\"\n",
        "    dot = np.dot(vocab, context)\n",
        "    prob = _softmax(dot / temperature)\n",
        "    return prob\n",
        "\n",
        "def prepare_topics(weights, factors, word_vectors, vocab, temperature=1.0,\n",
        "                   doc_lengths=None, term_frequency=None, normalize=False):\n",
        "    \"\"\" Collects a dictionary of word, document and topic distributions.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    weights : float array\n",
        "        This must be an array of unnormalized log-odds of document-to-topic\n",
        "        weights. Shape should be [n_documents, n_topics]\n",
        "    factors : float array\n",
        "        Should be an array of topic vectors. These topic vectors live in the\n",
        "        same space as word vectors and will be used to find the most similar\n",
        "        words to each topic. Shape should be [n_topics, n_dim].\n",
        "    word_vectors : float array\n",
        "        This must be a matrix of word vectors. Should be of shape\n",
        "        [n_words, n_dim]\n",
        "    vocab : list of str\n",
        "        These must be the strings for words corresponding to\n",
        "        indices [0, n_words]\n",
        "    temperature : float\n",
        "        Used to calculate the log probability of a word. Higher\n",
        "        temperatures make more rare words more likely.\n",
        "    doc_lengths : int array\n",
        "        An array indicating the number of words in the nth document.\n",
        "        Must be of shape [n_documents]. Required by pyLDAvis.\n",
        "    term_frequency : int array\n",
        "        An array indicating the overall number of times each token appears\n",
        "        in the corpus. Must be of shape [n_words]. Required by pyLDAvis.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    data : dict\n",
        "        This dictionary is readily consumed by pyLDAVis for topic\n",
        "        visualization.\n",
        "    \"\"\"\n",
        "    # Map each factor vector to a word\n",
        "    topic_to_word = []\n",
        "    msg = \"Vocabulary size did not match size of word vectors\"\n",
        "    assert len(vocab) == word_vectors.shape[0], msg\n",
        "    if normalize:\n",
        "        word_vectors /= np.linalg.norm(word_vectors, axis=1)[:, None]\n",
        "    # factors = factors / np.linalg.norm(factors, axis=1)[:, None]\n",
        "    for factor_vector in factors:\n",
        "        factor_to_word = prob_words(factor_vector, word_vectors,\n",
        "                                    temperature=temperature)\n",
        "        topic_to_word.append(np.ravel(factor_to_word))\n",
        "    topic_to_word = np.array(topic_to_word)\n",
        "    msg = \"Not all rows in topic_to_word sum to 1\"\n",
        "    assert np.allclose(np.sum(topic_to_word, axis=1), 1), msg\n",
        "    # Collect document-to-topic distributions, e.g. theta\n",
        "    doc_to_topic = _softmax_2d(weights)\n",
        "    msg = \"Not all rows in doc_to_topic sum to 1\"\n",
        "    assert np.allclose(np.sum(doc_to_topic, axis=1), 1), msg\n",
        "    data = {'topic_term_dists': topic_to_word,\n",
        "            'doc_topic_dists': doc_to_topic,\n",
        "            'doc_lengths': doc_lengths,\n",
        "            'vocab': vocab,\n",
        "            'term_frequency': term_frequency}\n",
        "    return data\n",
        "\n",
        "def print_top_words_per_topic(data, top_n=10, do_print=True):\n",
        "    \"\"\" Given a pyLDAvis data array, print out the top words in every topic.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    data : dict\n",
        "        A dict object that summarizes topic data and has been made using\n",
        "        `prepare_topics`.\n",
        "    \"\"\"\n",
        "    msgs = []\n",
        "    lists = []\n",
        "    for j, topic_to_word in enumerate(data['topic_term_dists']):\n",
        "        top = np.argsort(topic_to_word)[::-1][:top_n]\n",
        "        prefix = \"Top words in topic %i \" % j\n",
        "        top_words = [data['vocab'][i].strip().replace(' ', '_') for i in top]\n",
        "        msg = ' '.join(top_words)\n",
        "        if do_print:\n",
        "            print(prefix + msg)\n",
        "        lists.append(top_words)\n",
        "    return lists\n",
        "\n",
        "def get_request(url):\n",
        "    for _ in range(5):\n",
        "        try:\n",
        "            return float(requests.get(url).text)\n",
        "        except:\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "def topic_coherence(lists, services=['ca', 'cp', 'cv', 'npmi', 'uci',\n",
        "                                     'umass']):\n",
        "    \"\"\" Requests the topic coherence from AKSW Palmetto\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    lists : list of lists\n",
        "        A list of lists with one list of top words for each topic.\n",
        "\n",
        "    >>> topic_words = [['cake', 'apple', 'banana', 'cherry', 'chocolate']]\n",
        "    >>> topic_coherence(topic_words, services=['cv'])\n",
        "    {(0, 'cv'): 0.5678879445677241}\n",
        "    \"\"\"\n",
        "    url = u'http://palmetto.aksw.org/palmetto-webapp/service/{}?words={}'\n",
        "    reqs = [url.format(s, '%20'.join(top[:10])) for s in services for top in lists]\n",
        "    pool = multiprocessing.Pool()\n",
        "    coherences = pool.map(get_request, reqs)\n",
        "    pool.close()\n",
        "    pool.terminate()\n",
        "    pool.join()\n",
        "    del pool\n",
        "    args = [(j, s, top) for s in services for j, top in enumerate(lists)]\n",
        "    ans = {}\n",
        "    for ((j, s, t), tc) in zip(args, coherences):\n",
        "        ans[(j, s)] = tc\n",
        "    return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtuBylhgAZcu"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "class Tracking:\n",
        "    cache = {}\n",
        "    calls = 0\n",
        "    slope = 0.0\n",
        "\n",
        "    def __init__(self, n=5000):\n",
        "        \"\"\" The tracking class keeps a most recently used cache of values\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n: int\n",
        "        Number of items to keep.\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "\n",
        "    def add(self, key, item):\n",
        "        \"\"\" Add an item with a particular to the cache.\n",
        "\n",
        "        >>> tracker = Tracking()\n",
        "        >>> tracker.add('log_perplexity', 55.6)\n",
        "        >>> tracker.cache['log_perplexity']\n",
        "        [55.6]\n",
        "        >>> tracker.add('log_perplexity', 55.2)\n",
        "        >>> tracker.add('loss', -12.1)\n",
        "        >>> tracker.cache['log_perplexity']\n",
        "        [55.6, 55.2]\n",
        "        >>> tracker.cache['loss']\n",
        "        [-12.1]\n",
        "        \"\"\"\n",
        "        if key not in self.cache:\n",
        "            self.cache[key] = []\n",
        "        self.cache[key].append(item)\n",
        "        if len(self.cache[key]) > self.n:\n",
        "            self.cache[key] = self.cache[key][:self.n]\n",
        "\n",
        "    def stats(self, key):\n",
        "        \"\"\" Get the statistics for items with a particular key\n",
        "\n",
        "        >>> tracker = Tracking()\n",
        "        >>> tracker.add('log_perplexity', 55.6)\n",
        "        >>> tracker.add('log_perplexity', 55.2)\n",
        "        >>> tracker.stats('log_perplexity')\n",
        "        (55.400000000000006, 0.19999999999999929, 0.0)\n",
        "        \"\"\"\n",
        "        data = self.cache[key]\n",
        "        mean = np.mean(data)\n",
        "        std = np.std(data)\n",
        "        slope = self.slope\n",
        "        if self.calls % 100 == 0:\n",
        "            lr = LinearRegression()\n",
        "            x = np.arange(len(data)).astype('float32')\n",
        "            lr.fit(x[:, None], np.array(data))\n",
        "            self.slope = lr.coef_[0]\n",
        "        self.calls += 1\n",
        "        return mean, std, slope\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     import doctest\n",
        "#     doctest.testmod()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPPOd3qUA9gX"
      },
      "outputs": [],
      "source": [
        "from chainer import Variable\n",
        "import random\n",
        "\n",
        "def move(xp, *args):\n",
        "    for arg in args:\n",
        "        if 'float' in str(arg.dtype):\n",
        "            yield Variable(xp.asarray(arg, dtype='float32'))\n",
        "        else:\n",
        "            assert 'int' in str(arg.dtype)\n",
        "            yield Variable(xp.asarray(arg, dtype='int32'))\n",
        "\n",
        "def most_similar(embeddings, word_index):\n",
        "    input_vector = embeddings.W[word_index]\n",
        "    similarities = embeddings.dot(input_vector)\n",
        "    return similarities\n",
        "\n",
        "def chunks(n, *args):\n",
        "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
        "    # From stackoverflow question 312443\n",
        "    keypoints = []\n",
        "    for i in xrange(0, len(args[0]), n):\n",
        "        keypoints.append((i, i + n))\n",
        "    random.shuffle(keypoints)\n",
        "    for a, b in keypoints:\n",
        "        yield [arg[a: b] for arg in args]\n",
        "\n",
        "class MovingAverage():\n",
        "    def __init__(self, lastn=100):\n",
        "        self.points = np.array([])\n",
        "        self.lastn = lastn\n",
        "\n",
        "    def add(self, x):\n",
        "        self.points = np.append(self.points, x)\n",
        "\n",
        "    def mean(self):\n",
        "        return np.mean(self.points[-self.lastn:])\n",
        "\n",
        "    def std(self):\n",
        "        return np.std(self.points[-self.lastn:])\n",
        "\n",
        "    def get_stats(self):\n",
        "        return (np.mean(self.points[-self.lastn:]),\n",
        "                np.std(self.points[-self.lastn:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "P5MCoUH9BQ8F",
        "outputId": "f95b3391-3255-4dc8-caf4-a7e3da49655a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\t', 'en', '\\n', ' ', \"'\", \"''\", '\"', \"'Cause\", 'because', \"'cause\", 'use', \"'Xxxxx\", 'Cause', 'cause', 'C', 'Xxxxx', \"'Cos\", \"'cos\", 'Cos', \"'Xxx\", 'cos', 'Xxx', \"'Coz\", \"'coz\", 'Coz', 'coz', \"'Cuz\", \"'cuz\", 'Cuz', 'cuz', \"'S\", \"'s\", \"'X\", 'S', 's', \"'bout\", 'about', 'out', \"'xxxx\", 'bout', 'b', 'xxxx', 'c', \"'xxx\", 'xxx', \"'d\", \"'x\", 'd', 'x', \"'em\", 'them', \"'xx\", 'em', 'e', 'xx', \"'ll\", 'will', 'll', 'l', \"'nuff\", 'enough', 'uff', 'nuff', 'n', \"'re\", 'are', 're', 'r', '(*_*)', '(', '_*)', ')', '*', '(-8', '(-d', '-8', '-', '-d', '(-:', ':', '(-;', ';', '(-_-)', '_-)', '-_-', '(._.)', '_.)', '.', '(:', '(;', '(=', '=', '(>_<)', '_<)', '>', '<', '(^_^)', '_^)', '^_^', '^', '(o:', '(x:', 'o', '(_)', '_)', '_', '', '(_)', '_)', '(x_x)', '_', '', 'x_x', '(', '', '', '', '', '', '', '', '', ')-:', '):', '-__-', '__-', '._.', '0.0', '0', 'd.d', '0.o', 'd.x', '0_0', 'd_d', '0_o', 'd_x', '10', '1', 'dd', 'a.m.', 'a', '.m.', 'x.x.', '10a.m', 'a.m', 'ddx.x', '10a.m.', 'ddx.x.', 'am', 'p.m.', 'p', '10p.m', 'p.m', '10p.m.', 'pm', '11', '11a.m', '11a.m.', '11p.m', '11p.m.', '12', '12a.m', '12a.m.', '12p.m', '12p.m.', '1a.m', 'dx.x', '1a.m.', 'dx.x.', '1p.m', '1p.m.', '2', '2a.m', '2a.m.', '2p.m', '2p.m.', '3', '3a.m', '3a.m.', '3p.m', '3p.m.', '4', '4a.m', '4a.m.', '4p.m', '4p.m.', '5', '5a.m', '5a.m.', '5p.m', '5p.m.', '6', '6a.m', '6a.m.', '6p.m', '6p.m.', '7', '7a.m', '7a.m.', '7p.m', '7p.m.', '8)', '8', 'd)', '8-)', 'd-)', '8-', 'd-', '8-D', '8-d', 'd-X', 'D', '8D', '8d', 'dX', '8a.m', '8a.m.', '8p.m', '8p.m.', '9', '9a.m', '9a.m.', '9p.m', '9p.m.', \":'(\", \":')\", \":'-(\", \"'-(\", \":'-)\", \"'-)\", ':(', ':((', ':(((', '(((', ':()', ':)', ':))', ':)))', ')))', ':*', ':-(', ':-((', '-((', ':-(((', ':-)', ':-))', '-))', ':-)))', ':-*', ':-/', '-/', ':-0', ':-d', '-0', ':-3', '-3', ':->', ':-D', ':-X', '-D', '-X', ':-O', ':-o', '-O', '-o', ':-P', ':-p', '-P', '-p', ':-x', '-x', ':-]', ']', ':-|', '-|', ':-}', '}', ':/', '/', ':0', ':d', ':1', ':3', ':>', ':D', ':X', ':O', ':o', 'O', ':P', ':p', 'P', ':x', ':]', ':o)', ':x)', ':|', '|', ':}', ':(', '', ':)', ':-(', '-(', ':-)', '-)', ';)', ';-)', ';-D', ';-d', ';-X', ';D', ';d', ';X', ';_;', '<.<', '</3', '</d', '/3', '/d', '<3', '<d', '<33', '<dd', '33', '<333', '333', '<ddd', 'ddd', '<space>', 'ce>', '<xxxx>', 'space', 'ace', '=(', '=)', '=/', '=3', '=d', '=D', '=X', '=[', '[', '=]', '=|', '>.<', '>.>', '>:(', '>:o', '>:x', '><(((*>', '(*>', '@_@', '@', 'Adm.', 'adm.', 'A', 'dm.', 'Xxx.', 'Adm', 'adm', 'Ai', 'ai', 'Xx', \"n't\", 'not', \"x'x\", 'nt', 'nt', 'xx', 'Ak.', 'Alaska', 'ak.', 'Xx.', 'Ak', 'ak', 'Ala.', 'Alabama', 'ala.', 'la.', 'Ala', 'ala', 'Apr.', 'April', 'apr.', 'pr.', 'Apr', 'apr', 'Are', 'Ariz.', 'Arizona', 'ariz.', 'iz.', 'Xxxx.', 'Ariz', 'ariz', 'riz', 'Xxxx', 'Ark.', 'Arkansas', 'ark.', 'rk.', 'Ark', 'ark', 'Aug.', 'August', 'aug.', 'ug.', 'Aug', 'aug', 'Bros.', 'bros.', 'B', 'os.', 'Bros', 'bros', 'ros', \"C'm\", 'come', \"c'm\", \"X'x\", 'on', 'C++', 'c++', 'X++', 'Calif.', 'California', 'calif.', 'if.', 'Xxxxx.', 'Calif', 'calif', 'lif', 'Ca', 'can', 'ca', \"'ve\", 'have', 'Can', 've', 'v', 've', 'xx', 'Co.', 'co.', 'Co', 'co', 'Colo.', 'Colorado', 'colo.', 'lo.', 'Colo', 'colo', 'olo', 'Conn.', 'Connecticut', 'conn.', 'nn.', 'Conn', 'conn', 'onn', 'Corp.', 'corp.', 'rp.', 'Corp', 'corp', 'orp', 'Could', 'could', 'uld', 'Cm', 'cm', 'Xx', 'D.C.', 'd.c.', '.C.', 'X.X.', 'Dare', 'dare', 'Dec.', 'December', 'dec.', 'ec.', 'Dec', 'dec', 'Del.', 'Delaware', 'del.', 'el.', 'Del', 'del', 'Did', 'do', 'did', 'Does', 'does', 'oes', 'Doin', 'doing', 'doin', 'oin', \"Doin'\", \"doin'\", \"in'\", \"Xxxx'\", 'Doin', 'doin', 'in', 'Xxxx', 'Do', 'Dr.', 'dr.', 'Dr', 'dr', 'E.G.', 'e.g.', 'E', '.G.', 'E.g.', '.g.', 'X.x.', 'E.g', 'e.g', 'X.x', 'Feb.', 'February', 'feb.', 'F', 'eb.', 'Feb', 'feb', 'Fla.', 'Florida', 'fla.', 'Fla', 'fla', 'Ga.', 'Georgia', 'ga.', 'G', 'Ga', 'ga', 'Gen.', 'gen.', 'en.', 'Gen', 'gen', 'Goin', 'going', 'goin', \"Goin'\", \"goin'\", 'Goin', 'goin', 'Gon', 'gon', 'na', 'to', 'Got', 'got', 'ta', 't', 'Gov.', 'gov.', 'ov.', 'Gov', 'gov', 'Had', 'had', 'H', 'Has', 'has', 'Have', 'ave', 'Havin', 'having', 'havin', 'vin', \"Havin'\", \"havin'\", \"Xxxxx'\", 'Havin', 'havin', 'Xxxxx', 'He', 'he', 'would', \"He's\", \"he's\", \"e's\", \"Xx'x\", 'd', 'x', 'll', 's', 'Hes', 'hes', 'es', 'Xxx', 'How', 'how', \"'y\", 'you', \"How's\", \"how's\", \"w's\", \"Xxx'x\", 'y', 're', 'Hows', 'hows', 'ws', 'Xxxx', 'I', 'i', \"'m\", 'gonna', 'I.E.', 'i.e.', '.E.', 'I.e.', '.e.', 'I.e', 'i.e', 'Ia.', 'Iowa', 'ia.', 'Ia', 'ia', 'Id.', 'Idaho', 'id.', 'Id', 'id', 'Ill.', 'Illinois', 'ill.', 'll.', 'Ill', 'ill', 'm', 'Inc.', 'inc.', 'nc.', 'Inc', 'inc', 'Ind.', 'Indiana', 'ind.', 'nd.', 'Ind', 'ind', 'Is', 'is', 'It', 'it', \"It's\", \"it's\", \"t's\", 'Its', 'its', 'ts', 'm', 'Jan.', 'January', 'jan.', 'J', 'an.', 'Jan', 'jan', 'Jr.', 'jr.', 'Jr', 'jr', 'Jul.', 'July', 'jul.', 'ul.', 'Jul', 'jul', 'Jun.', 'June', 'jun.', 'un.', 'Jun', 'jun', 'Kan.', 'Kansas', 'kan.', 'K', 'Kan', 'kan', 'Kans.', 'kans.', 'ns.', 'Kans', 'kans', 'ans', 'Ky.', 'Kentucky', 'ky.', 'Ky', 'ky', 'La.', 'Louisiana', 'L', 'La', 'la', 'Let', 'let', 'us', \"Let's\", \"let's\", 'Lets', 'lets', 'Lovin', 'loving', 'lovin', \"Lovin'\", \"lovin'\", 'Lovin', 'lovin', 'Ltd.', 'ltd.', 'td.', 'Ltd', 'ltd', \"Ma'am\", 'madam', \"ma'am\", 'M', \"'am\", \"Xx'xx\", 'Mar.', 'March', 'mar.', 'ar.', 'Mar', 'mar', 'Mass.', 'Massachusetts', 'mass.', 'ss.', 'Mass', 'mass', 'ass', 'May', 'may', 'Maam', 'maam', 'am', 'Xxxx', 'Md.', 'md.', 'Md', 'md', 'Messrs.', 'messrs.', 'rs.', 'Messrs', 'messrs', 'srs', 'Mich.', 'Michigan', 'mich.', 'ch.', 'Mich', 'mich', 'ich', 'Might', 'might', 'ght', 'Minn.', 'Minnesota', 'minn.', 'Minn', 'minn', 'inn', 'Miss.', 'Mississippi', 'miss.', 'Miss', 'miss', 'iss', 'Mo.', 'mo.', 'Mo', 'mo', 'Mont.', 'mont.', 'nt.', 'Mont', 'mont', 'ont', 'Mr.', 'mr.', 'Mr', 'mr', 'Mrs.', 'mrs.', 'Mrs', 'mrs', 'Ms.', 'ms.', 'Ms', 'ms', 'Mt.', 'Mount', 'mt.', 'Mt', 'mt', 'Must', 'must', 'ust', 'N.C.', 'North Carolina', 'n.c.', 'N', 'N.D.', 'North Dakota', 'n.d.', '.D.', 'N.H.', 'New Hampshire', 'n.h.', '.H.', 'N.J.', 'New Jersey', 'n.j.', '.J.', 'N.M.', 'New Mexico', 'n.m.', '.M.', 'N.Y.', 'New York', 'n.y.', '.Y.', 'Neb.', 'Nebraska', 'neb.', 'Neb', 'neb', 'Nebr.', 'nebr.', 'br.', 'Nebr', 'nebr', 'ebr', 'Need', 'need', 'eed', 'Nev.', 'Nevada', 'nev.', 'ev.', 'Nev', 'nev', 'Not', 'Nothin', 'nothing', 'nothin', 'hin', \"Nothin'\", \"nothin'\", 'Nothin', 'nothin', 'Nov.', 'November', 'nov.', 'Nov', 'nov', 'Nuthin', 'nuthin', \"Nuthin'\", \"nuthin'\", 'Nuthin', 'nuthin', \"O'clock\", \"o'clock\", 'ock', \"X'xxxx\", 'O.O', 'o.o', 'X.X', 'O.o', 'O_O', 'o_o', 'X_X', 'O_o', 'X_x', 'Oct.', 'October', 'oct.', 'ct.', 'Oct', 'oct', 'Okla.', 'Oklahoma', 'okla.', 'Okla', 'okla', 'kla', 'Ol', 'old', 'ol', \"Ol'\", \"ol'\", \"Xx'\", 'Ol', 'ol', 'Xx', 'Ore.', 'Oregon', 'ore.', 're.', 'Ore', 'ore', 'Ought', 'ought', 'Oclock', 'oclock', 'Xxxxx', 'Pa.', 'Pennsylvania', 'pa.', 'Pa', 'pa', 'Ph.D.', 'ph.d.', 'Xx.X.', 'Ph', 'ph', 'D.', 'd.', 'X.', 'Prof.', 'prof.', 'of.', 'Prof', 'prof', 'rof', 'Rep.', 'rep.', 'R', 'ep.', 'Rep', 'rep', 'Rev.', 'rev.', 'Rev', 'rev', 'S.C.', 'South Carolina', 's.c.', 'Sen.', 'sen.', 'Sen', 'sen', 'Sep.', 'September', 'sep.', 'Sep', 'sep', 'Sept.', 'sept.', 'pt.', 'Sept', 'sept', 'ept', 'Sha', 'shall', 'sha', 'She', 'she', \"She's\", \"she's\", 'Shes', 'shes', 'Should', 'should', 'Somethin', 'something', 'somethin', \"Somethin'\", \"somethin'\", 'Somethin', 'somethin', 'St.', 'st.', 'St', 'st', 'Tenn.', 'Tennessee', 'tenn.', 'T', 'Tenn', 'tenn', 'enn', 'That', 'that', 'hat', \"That's\", \"that's\", \"Xxxx'x\", 'Thats', 'thats', 'Xxxxx', 'There', 'there', 'ere', \"There's\", \"there's\", \"Xxxxx'x\", 'Theres', 'theres', 'Xxxxxx', 'These', 'these', 'ese', 'They', 'they', 'hey', 'This', 'this', 'his', \"This's\", \"this's\", \"s's\", 'Thiss', 'thiss', 'ss', 'Those', 'those', 'ose', 'V.V', 'v.v', 'V', 'V_V', 'v_v', 'Va.', 'Virginia', 'va.', 'Va', 'va', 'Wash.', 'Washington', 'wash.', 'W', 'sh.', 'Wash', 'wash', 'ash', 'Was', 'was', 'We', 'we', 'Were', 'were', 'What', 'what', \"What's\", \"what's\", 'Whats', 'whats', 'When', 'when', 'hen', \"When's\", \"when's\", \"n's\", 'Whens', 'whens', 'ns', 'Where', 'where', \"Where's\", \"where's\", 'Wheres', 'wheres', 'Who', 'who', \"Who's\", \"who's\", \"o's\", 'Whos', 'whos', 'os', 'Why', 'why', \"Why's\", \"why's\", \"y's\", 'Whys', 'whys', 'ys', 'Wis.', 'Wisconsin', 'wis.', 'is.', 'Wis', 'wis', 'Wo', 'wo', 'Would', 'XD', 'xd', 'XX', 'XDD', 'xdd', 'XXX', 'You', 'Y', '[-:', '[:', '[=', '\\\\\")', '\\\\', '\\\\n', '\\\\x', '\\\\t', ']=', '^__^', '__^', '^___^', 'a.', 'x.', 'x.x', 'and/or', '/or', 'xxx/xx', 'and', 'or', 'b.', 'c.', 'xx.', \"xxxx'\", 'xxxx', 'e.', 'f.', 'f', 'g.', 'g', 'h.', 'h', \"xx'x\", 'xxx', \"xxx'x\", 'xxxx', 'i.', 'j.', 'j', 'k.', 'k', 'l.', 'm.', \"xx'xx\", 'xxxx', 'n.', \"x'xxxx\", 'o.', 'o.0', 'x.d', 'o.O', 'x.X', 'o_0', 'x_d', 'o_O', 'x_X', \"xx'\", 'xx', 'xxxxx', 'p.', 'q.', 'q', 'r.', 's.', 't.', \"xxxx'x\", 'xxxxx', 'u.', 'u', 'v.', 'v.s.', '.s.', 'v.s', 'vs.', 'vs', 'w.', 'w', 'w/o', 'without', 'x/x', 'xD', 'xX', 'xDD', 'xXX', \"y'\", 'y', \"x'\", 'all', 'y.', 'y', 'x', 'z.', 'z', '\\xa0', '  ', '\\\\()/', '', ')/', '\\\\(x)/', 'C.', 'C.', 'c.', 'X.', 'F.', 'F.', 'f.', 'K.', 'K.', 'k.', 'x.', '.', '', '.', '', '.', '', '', 'xx', '', 'S', 's', '', 'X', 'x', 'Cause', 'cause', 'Xxxxx', 'Cos', 'cos', 'Xxx', 'Coz', 'coz', 'Cuz', 'cuz', 'S', 'X', 'bout', 'xxxx', 'xxx', 'em', 'nuff', '']\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-142-332c36b0ae57>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'spacy.lexeme.Lexeme' object has no attribute 'string'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pytest\n",
        "import os\n",
        "\n",
        "on_ci = os.environ.get('CI', False) == 'true'\n",
        "nlp = English()\n",
        "\n",
        "@pytest.mark.skipif(on_ci, reason='SpaCy install fails on TravisCI')\n",
        "def test_tokenize():\n",
        "    texts = [u'Do you recall, not long ago']\n",
        "    texts += [u'We would walk on the sidewalk?']\n",
        "    arr, vocab = tokenize(texts, 10)\n",
        "    print(arr, vocab)\n",
        "    assert arr[0, 0] != arr[0, 1]\n",
        "    assert arr.shape[0] == 2\n",
        "    assert arr.shape[1] == 10\n",
        "    assert arr[0, -1] == -2\n",
        "    assert arr.dtype == np.dtype('int32')\n",
        "    first_word = texts[0].split(' ')[0].lower()\n",
        "    first_lowr = nlp.vocab[arr[0, 0]].lower_\n",
        "    print(first_word, \"hello\", first_lowr)\n",
        "    assert first_word == first_lowr\n",
        "\n",
        "test_tokenize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHkpS3mDPF7K"
      },
      "outputs": [],
      "source": [
        "\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (pytorchenv)",
      "language": "python",
      "name": "pytorchenv"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
